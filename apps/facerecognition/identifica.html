<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reconocimiento Facial con Webcam</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
    <style>
        /* Estilos adicionales para asegurar que el canvas se superponga correctamente */
        .video-container {
            position: relative;
            width: 100%;
            max-width: 720px; /* L√≠mite de ancho para el video */
            margin: auto; /* Centrar el contenedor */
        }
        #canvasOverlay {
            position: absolute;
            top: 0;
            left: 0;
            pointer-events: none; /* Para que los eventos del mouse pasen al video si es necesario */
        }
        /* Estilo para el spinner de carga */
        .loader {
            border: 5px solid #f3f3f3; /* Light grey */
            border-top: 5px solid #3498db; /* Blue */
            border-radius: 50%;
            width: 50px;
            height: 50px;
            animation: spin 1s linear infinite;
            margin: 20px auto;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
</head>
<body class="bg-gray-100 flex flex-col items-center justify-center min-h-screen p-4 font-sans">

    <div class="bg-white p-6 md:p-8 rounded-xl shadow-2xl w-full max-w-3xl">
        <header class="mb-6 text-center">
            <h1 class="text-3xl md:text-4xl font-bold text-blue-600">Reconocimiento Facial ü§≥</h1>
            <p class="text-gray-600 mt-2">Carga tu archivo JSON de descriptores y mira la magia.</p>
        </header>

        <div class="video-container mb-6 rounded-lg overflow-hidden shadow-lg border-2 border-blue-300">
            <video id="videoFeed" playsinline autoplay muted class="w-full h-auto block"></video>
            <canvas id="canvasOverlay" class="w-full h-auto"></canvas>
        </div>

        <div class="controls-status mb-6 p-4 bg-gray-50 rounded-lg border border-gray-200">
            <div class="flex flex-col sm:flex-row gap-4 mb-4">
                <div class="flex-1">
                     <label for="jsonFileInput" class="block text-sm font-medium text-gray-700 mb-1">
                        Cargar archivo <code>face_recognition_data.json</code>:
                    </label>
                    <input type="file" id="jsonFileInput" accept=".json"
                           class="block w-full text-sm text-gray-500
                                  file:mr-4 file:py-2 file:px-4
                                  file:rounded-lg file:border-0
                                  file:text-sm file:font-semibold
                                  file:bg-blue-50 file:text-blue-700
                                  hover:file:bg-blue-100
                                  cursor-pointer">
                </div>
                 <div class="flex-shrink-0">
                    <label class="block text-sm font-medium text-gray-700 mb-1">&nbsp;</label> <button id="switchCamButton" class="hidden w-full sm:w-auto bg-blue-500 hover:bg-blue-600 text-white font-bold py-2 px-4 rounded-lg transition-colors">
                        Cambiar C√°mara
                    </button>
                </div>
            </div>
            
            <div id="statusMessage" class="text-sm text-center p-3 rounded-md min-h-[50px] flex items-center justify-center">
                <div id="loader" class="loader hidden"></div>
                <span id="statusText" class="text-gray-700">Inicializando aplicaci√≥n...</span>
            </div>
        </div>

        <footer class="text-center text-xs text-gray-500">
            <p>&copy; 2024 Aplicaci√≥n de Reconocimiento Facial. Usando face-api.js.</p>
        </footer>
    </div>

    <script>
        // Referencias a elementos del DOM
        const videoElement = document.getElementById('videoFeed');
        const canvasElement = document.getElementById('canvasOverlay');
        const jsonFileInputElement = document.getElementById('jsonFileInput');
        const statusTextElement = document.getElementById('statusText');
        const loaderElement = document.getElementById('loader');
        const switchCamButton = document.getElementById('switchCamButton'); // <<< CAMBIO: Referencia al nuevo bot√≥n

        // Constantes y variables globales
        const MODEL_URL = 'https://cdn.jsdelivr.net/gh/justadudewhohacks/face-api.js@0.22.2/weights';
        let labeledFaceDescriptors = null;
        let faceMatcher = null;
        let modelsAreLoaded = false;
        let streamGlob = null;
        let recognitionInterval = null;
        let currentFacingMode = 'user'; // <<< CAMBIO: Variable para rastrear la c√°mara actual ('user' o 'environment')

        // --- 1. Funciones de Carga y Configuraci√≥n ---

        function updateStatus(message, isLoading = false, type = 'info') {
            statusTextElement.textContent = message;
            loaderElement.classList.toggle('hidden', !isLoading);
            statusTextElement.classList.toggle('hidden', isLoading && !message);

            const statusMessageContainer = document.getElementById('statusMessage');
            statusMessageContainer.className = 'text-sm text-center p-3 rounded-md min-h-[50px] flex items-center justify-center ';
            switch (type) {
                case 'success': statusMessageContainer.classList.add('bg-green-100', 'text-green-700'); break;
                case 'warning': statusMessageContainer.classList.add('bg-yellow-100', 'text-yellow-700'); break;
                case 'error': statusMessageContainer.classList.add('bg-red-100', 'text-red-700'); break;
                default: statusMessageContainer.classList.add('bg-blue-50', 'text-blue-700'); break;
            }
            console.log(`Status: ${message} (Type: ${type}, Loading: ${isLoading})`);
        }

        async function loadModels() {
            updateStatus("Cargando modelos de IA...", true, 'info');
            try {
                await Promise.all([
                    faceapi.nets.ssdMobilenetv1.loadFromUri(MODEL_URL),
                    faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
                    faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL)
                ]);
                modelsAreLoaded = true;
                updateStatus("Modelos cargados. ‚úÖ Inicia la webcam o carga tu JSON.", false, 'success');
            } catch (error) {
                console.error("Error al cargar los modelos de face-api.js:", error);
                updateStatus("Error al cargar modelos. üò≠ Revisa la consola.", false, 'error');
                modelsAreLoaded = false;
            }
        }

        /**
         * <<< CAMBIO: La funci√≥n ahora acepta 'facingMode' como par√°metro.
         * Inicia la webcam y la muestra en el elemento de video.
         * @param {string} facingMode - 'user' para la c√°mara frontal, 'environment' para la trasera.
         */
        async function startWebcam(facingMode) {
            if (!modelsAreLoaded) {
                updateStatus("Espera a que los modelos carguen antes de iniciar la webcam.", false, 'warning');
                return;
            }
            updateStatus("Accediendo a la webcam...", true, 'info');
            try {
                if (streamGlob) {
                    streamGlob.getTracks().forEach(track => track.stop());
                }
                
                const constraints = {
                    video: { facingMode: { exact: facingMode } }, // <<< CAMBIO: Usa el par√°metro
                    audio: false
                };
                streamGlob = await navigator.mediaDevices.getUserMedia(constraints);
                
                videoElement.srcObject = streamGlob;
                currentFacingMode = facingMode; // <<< CAMBIO: Actualiza el estado actual
                videoElement.onloadedmetadata = () => {
                    videoElement.play();
                    updateStatus("Webcam activa. üé• Carga tu archivo JSON para reconocer.", false, 'success');
                    faceapi.matchDimensions(canvasElement, videoElement, true);
                }
            } catch (err) {
                console.error("Error al acceder a la webcam:", err);
                let errorMessage = "Error al acceder a la webcam. üö´ Verifica permisos y conexi√≥n.";
                if (err.name === "NotAllowedError") {
                     errorMessage = "Permiso para la webcam denegado. Por favor, habil√≠talo.";
                } else if (err.name === "NotFoundError" || err.name === "OverconstrainedError") {
                     errorMessage = `No se encontr√≥ una c√°mara ${facingMode === 'user' ? 'frontal' : 'posterior'}.`;
                }
                updateStatus(errorMessage, false, 'error');
            }
        }

        // --- 2. Funciones de Procesamiento del JSON (sin cambios) ---
        async function handleJsonFile(event) {
            if (!modelsAreLoaded) {
                updateStatus("Modelos no cargados. Espera e intenta de nuevo.", false, 'warning');
                jsonFileInputElement.value = "";
                return;
            }
            if (!event.target.files || event.target.files.length === 0) {
                updateStatus("No se seleccion√≥ ning√∫n archivo.", false, 'warning');
                return;
            }
            const file = event.target.files[0];
            updateStatus(`Procesando archivo: ${file.name}...`, true, 'info');
            const reader = new FileReader();
            reader.onload = async (e) => {
                try {
                    const jsonString = e.target.result;
                    const parsedData = JSON.parse(jsonString);
                    if (!Array.isArray(parsedData)) throw new Error("El formato del JSON no es un array.");
                    if (parsedData.length === 0) {
                        updateStatus("El archivo JSON est√° vac√≠o.", false, 'warning');
                        return;
                    }
                    labeledFaceDescriptors = parsedData.map(person => {
                        if (!person.label || !Array.isArray(person.descriptors)) throw new Error(`Registro de persona inv√°lido para "${person.label || 'desconocido'}".`);
                        const descriptorsAsFloat32Array = person.descriptors.map(d => new Float32Array(d));
                        return new faceapi.LabeledFaceDescriptors(person.label, descriptorsAsFloat32Array);
                    });
                    console.log("Descriptores faciales cargados:", labeledFaceDescriptors);
                    if (labeledFaceDescriptors.length > 0) {
                        faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.55);
                        updateStatus(`¬°Listo! ${labeledFaceDescriptors.length} persona(s) cargada(s). ‚ú®`, false, 'success');
                        if (videoElement.paused && videoElement.srcObject) videoElement.play();
                    } else {
                        faceMatcher = null;
                        updateStatus("No se cargaron descriptores v√°lidos.", false, 'warning');
                    }
                } catch (error) {
                    console.error("Error al procesar el JSON:", error);
                    updateStatus(`Error al procesar JSON: ${error.message}. üòû`, false, 'error');
                }
            };
            reader.onerror = (error) => updateStatus("Error al leer el archivo JSON.", false, 'error');
            reader.readAsText(file);
        }

        // --- 3. Funciones de Reconocimiento Facial (sin cambios) ---
        function startRecognition() {
            if (recognitionInterval) clearInterval(recognitionInterval);
            recognitionInterval = setInterval(async () => {
                if (!videoElement || videoElement.paused || !faceMatcher || !modelsAreLoaded) {
                    if (canvasElement.getContext) canvasElement.getContext('2d').clearRect(0, 0, canvasElement.width, canvasElement.height);
                    return;
                }
                if (canvasElement.width !== videoElement.videoWidth || canvasElement.height !== videoElement.videoHeight) {
                    faceapi.matchDimensions(canvasElement, videoElement, true);
                }
                const displaySize = { width: videoElement.videoWidth, height: videoElement.videoHeight };
                const detections = await faceapi.detectAllFaces(videoElement, new faceapi.SsdMobilenetv1Options({ minConfidence: 0.5 }))
                    .withFaceLandmarks()
                    .withFaceDescriptors();
                const resizedDetections = faceapi.resizeResults(detections, displaySize);
                const context = canvasElement.getContext('2d');
                context.clearRect(0, 0, canvasElement.width, canvasElement.height);
                resizedDetections.forEach(detection => {
                    const bestMatch = faceMatcher.findBestMatch(detection.descriptor);
                    const label = bestMatch.distance > 0.55 ? `Desconocido (${bestMatch.distance.toFixed(2)})` : `${bestMatch.label} (${bestMatch.distance.toFixed(2)})`;
                    const box = detection.detection.box;
                    const drawBox = new faceapi.draw.DrawBox(box, {
                        label: label,
                        boxColor: 'rgba(0, 255, 0, 0.7)',
                        drawLabelOptions: {
                            fontColor: 'white',
                            fontSize: 16,
                            padding: 4,
                            backgroundColor: 'rgba(0, 150, 0, 0.7)'
                        }
                    });
                    drawBox.draw(canvasElement);
                });
            }, 150);
        }

        // --- 4. Inicializaci√≥n y Event Listeners ---

        jsonFileInputElement.addEventListener('change', handleJsonFile);
        videoElement.addEventListener('play', () => {
            if (!modelsAreLoaded) {
                updateStatus("Modelos a√∫n no cargados.", false, 'warning');
                return;
            }
            if (!faceMatcher) {
                 updateStatus("Webcam activa. Carga un JSON para reconocer.", false, 'info');
            }
            startRecognition();
        });
        videoElement.addEventListener('loadedmetadata', () => {
            faceapi.matchDimensions(canvasElement, videoElement, true);
        });

        // <<< CAMBIO: Event listener para el bot√≥n de cambiar c√°mara
        switchCamButton.addEventListener('click', () => {
            const newFacingMode = currentFacingMode === 'user' ? 'environment' : 'user';
            updateStatus(`Cambiando a c√°mara ${newFacingMode === 'user' ? 'frontal' : 'posterior'}...`, true, 'info');
            startWebcam(newFacingMode);
        });

        // <<< CAMBIO: La funci√≥n de inicializaci√≥n ahora revisa las c√°maras disponibles
        async function initializeApp() {
            await loadModels();
            
            // Revisa si hay m√∫ltiples c√°maras y muestra el bot√≥n si es necesario
            try {
                const devices = await navigator.mediaDevices.enumerateDevices();
                const videoDevices = devices.filter(device => device.kind === 'videoinput');
                if (videoDevices.length > 1) {
                    switchCamButton.classList.remove('hidden');
                    console.log(`Se encontraron ${videoDevices.length} c√°maras.`);
                } else {
                    console.log("Solo se encontr√≥ una c√°mara, el bot√≥n de cambio permanecer√° oculto.");
                }
            } catch (err) {
                console.error("Error enumerando dispositivos, no se mostrar√° el bot√≥n de cambio de c√°mara.", err);
            }

            await startWebcam(currentFacingMode); // Inicia con la c√°mara frontal por defecto
        }

        document.addEventListener('DOMContentLoaded', initializeApp);

    </script>
</body>
</html>

